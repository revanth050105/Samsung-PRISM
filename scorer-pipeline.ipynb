{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "System version: 3.7.9 (tags/v3.7.9:13c94747c7, Aug 17 2020, 18:58:18) [MSC v.1900 64 bit (AMD64)] <br>\n",
    "Tensorflow version: 2.6.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import zipfile\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import re\n",
    "import random\n",
    "from tempfile import TemporaryDirectory\n",
    "import tensorflow as tf\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "from recommenders.models.deeprec.deeprec_utils import download_deeprec_resources \n",
    "from recommenders.models.newsrec.newsrec_utils import prepare_hparams\n",
    "from recommenders.models.newsrec.models.naml import NAMLModel\n",
    "from recommenders.models.newsrec.io.mind_all_iterator import MINDAllIterator\n",
    "from recommenders.models.newsrec.newsrec_utils import get_mind_data_set\n",
    "from recommenders.utils.notebook_utils import store_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"...\"\n",
    "train_news_file = os.path.join(data_path, 'train', r'news.tsv')\n",
    "train_behaviors_file = os.path.join(data_path, 'train', r'behaviors.tsv')\n",
    "valid_news_file = os.path.join(data_path, 'valid', r'news.tsv')\n",
    "valid_behaviors_file = os.path.join(data_path, 'valid', r'behaviors.tsv')\n",
    "wordEmb_file = os.path.join(data_path, \"utils\", \"embedding_all.npy\")\n",
    "userDict_file = os.path.join(data_path, \"utils\", \"uid2index.pkl\")\n",
    "wordDict_file = os.path.join(data_path, \"utils\", \"word_dict_all.pkl\")\n",
    "vertDict_file = os.path.join(data_path, \"utils\", \"vert_dict.pkl\")\n",
    "subvertDict_file = os.path.join(data_path, \"utils\", \"subvert_dict.pkl\")\n",
    "yaml_file = os.path.join(data_path, \"utils\", r'naml.yaml')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model path\n",
    "model_path = \"...\"\n",
    "\n",
    "with open(subvertDict_file, \"rb\") as f:\n",
    "    subvert_dict = pickle.load(f)\n",
    "\n",
    "\n",
    "with open(vertDict_file, \"rb\") as f:\n",
    "    vert_dict = pickle.load(f)\n",
    "    \n",
    "hparams = prepare_hparams(yaml_file,\n",
    "                          vert_num=max(vert_dict.values()) + 1,\n",
    "                          subvert_num=max(subvert_dict.values()) + 1,\n",
    "                          batch_size=32,\n",
    "                          epochs=5,\n",
    "                          wordEmb_file=wordEmb_file,\n",
    "                          wordDict_file=wordDict_file, \n",
    "                          userDict_file=userDict_file,\n",
    "                          vertDict_file=vertDict_file, \n",
    "                          subvertDict_file=subvertDict_file)\n",
    "\n",
    "iterator = MINDAllIterator\n",
    "model = NAMLModel(hparams, iterator, seed=42)\n",
    "model.model.load_weights(os.path.join(model_path, \"naml_ckpt\"))\n",
    "\n",
    "scorer = model._build_graph()[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_file = train_news_file\n",
    "behaviors_file = train_behaviors_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated CSV with cleaned headlines saved to .\\generated_headlines_r1.csv\n"
     ]
    }
   ],
   "source": [
    "generated_csv = \".\\\\generated_headlines_r1.csv\"\n",
    "results_df = pd.read_csv(generated_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(wordDict_file, \"rb\") as f:\n",
    "    word_dict = pickle.load(f)\n",
    "\n",
    "with open(vertDict_file, \"rb\") as f:\n",
    "    vert_dict = pickle.load(f)\n",
    "\n",
    "with open(subvertDict_file, \"rb\") as f:\n",
    "    subvert_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HParams:\n",
    "    his_size = 50\n",
    "    title_size = 30\n",
    "    body_size = 50\n",
    "\n",
    "hparams = HParams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text, max_length):\n",
    "    \"\"\"Read and Process news Data\"\"\"\n",
    "    text = str(text).replace('\"', '').replace(':', '').replace('-', ' ').replace(',', '').replace(\"'\", ' ').replace(\";\", '').replace('.', '').replace('?', '').replace(\"â\",\"\").replace(\"€\",\"\").replace(\"™\",\"\")\n",
    "    tokens = text.lower().split()[:max_length]\n",
    "    token_ids = [word_dict.get(token, 0) for token in tokens]\n",
    "    return token_ids + [0] * (max_length - len(token_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_news(news_df):\n",
    "    \"\"\"Convert news to numerical format for NAML model\"\"\"\n",
    "    news_dict = {}\n",
    "    for _, row in news_df.iterrows():\n",
    "        news_dict[row[\"news_id\"]] = {\n",
    "            \"title\": tokenize_text(row[\"title\"], hparams.title_size),\n",
    "            \"body\": tokenize_text(row[\"abstract\"], hparams.body_size),\n",
    "            \"category\": vert_dict.get(row[\"category\"], 0),\n",
    "            \"subcategory\": subvert_dict.get(row[\"subcategory\"], 0),\n",
    "        }\n",
    "    return news_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df = pd.read_csv(news_file, sep=\"\\t\", header=None,\n",
    "                      names=[\"news_id\", \"category\", \"subcategory\", \"title\", \"abstract\",\n",
    "                             \"url\", \"title_entities\", \"abstract_entities\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_dict = convert_news(news_df)\n",
    "behaviors_df = pd.read_csv(behaviors_file, sep=\"\\t\", header=None,\n",
    "                           names=[\"impression_id\", \"user_id\", \"timestamp\", \"history\", \"impressions\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_click_probability(user_id, news_title_tokens):\n",
    "    \"\"\" Compute click probability for a given user and news title \"\"\"\n",
    "    user_data = behaviors_df[behaviors_df[\"user_id\"] == user_id]\n",
    "    if not user_data.empty and isinstance(user_data[\"history\"].values[0], str):\n",
    "        user_clicked_news = user_data[\"history\"].values[0].split()[-hparams.his_size:]\n",
    "    else:\n",
    "        user_clicked_news = []\n",
    "\n",
    "    clicked_title_batch = [news_dict[n][\"title\"] for n in user_clicked_news if n in news_dict]\n",
    "    clicked_body_batch = [news_dict[n][\"body\"] for n in user_clicked_news if n in news_dict]\n",
    "    clicked_vert_batch = [[news_dict[n][\"category\"]] for n in user_clicked_news if n in news_dict]\n",
    "    clicked_subvert_batch = [[news_dict[n][\"subcategory\"]] for n in user_clicked_news if n in news_dict]\n",
    "\n",
    "    while len(clicked_title_batch) < hparams.his_size:\n",
    "        clicked_title_batch.append([0] * hparams.title_size)\n",
    "        clicked_body_batch.append([0] * hparams.body_size)\n",
    "        clicked_vert_batch.append([0])\n",
    "        clicked_subvert_batch.append([0])\n",
    "\n",
    "    clicked_title_batch = np.array(clicked_title_batch).reshape(1, hparams.his_size, hparams.title_size)\n",
    "    clicked_body_batch = np.array(clicked_body_batch).reshape(1, hparams.his_size, hparams.body_size)\n",
    "    clicked_vert_batch = np.array(clicked_vert_batch).reshape(1, hparams.his_size, 1)\n",
    "    clicked_subvert_batch = np.array(clicked_subvert_batch).reshape(1, hparams.his_size, 1)\n",
    "\n",
    "    candidate_title_batch = np.array(news_title_tokens).reshape(1, 1, hparams.title_size)\n",
    "    candidate_body_batch = np.zeros((1, 1, hparams.body_size))  # placeholders for news body and category since we're evaluating titles\n",
    "    candidate_vert_batch = np.zeros((1, 1, 1))\n",
    "    candidate_subvert_batch = np.zeros((1, 1, 1))\n",
    "\n",
    "    score = scorer.predict([\n",
    "        clicked_title_batch,\n",
    "        clicked_body_batch,\n",
    "        clicked_vert_batch,\n",
    "        clicked_subvert_batch,\n",
    "        candidate_title_batch,\n",
    "        candidate_body_batch,\n",
    "        candidate_vert_batch,\n",
    "        candidate_subvert_batch\n",
    "    ])\n",
    "\n",
    "    return float(score.flatten()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.read_csv(generated_csv)\n",
    "\n",
    "original_probs = []\n",
    "generated_probs = []\n",
    "\n",
    "for _, row in results_df.iterrows():\n",
    "    user_id = row[\"user_id\"]\n",
    "    article_id = row[\"article_id\"]\n",
    "\n",
    "    original_tokens = tokenize_text(row[\"original_headline\"], hparams.title_size)\n",
    "    \n",
    "    generated_tokens = tokenize_text(row[\"cleaned_headline\"], hparams.title_size)\n",
    "\n",
    "    original_prob = get_click_probability(user_id, original_tokens)\n",
    "    generated_prob = get_click_probability(user_id, generated_tokens)\n",
    "\n",
    "    original_probs.append(original_prob)\n",
    "    generated_probs.append(generated_prob)\n",
    "\n",
    "results_df[\"original_click_prob\"] = original_probs\n",
    "results_df[\"generated_click_prob\"] = generated_probs\n",
    "\n",
    "results_df.to_csv(generated_csv, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Click Probability Difference (Generated - Original): 0.004471528778473536\n",
      "Average Percentage Improvement: 0.859860333011391 %\n",
      "Win Rate (% of cases where generated headline wins): 70.20833333333333 %\n"
     ]
    }
   ],
   "source": [
    "results_df = pd.read_csv(generated_csv)\n",
    "\n",
    "results_df[\"diff\"] = results_df[\"generated_click_prob\"] - results_df[\"original_click_prob\"]\n",
    "mean_diff = results_df[\"diff\"].mean()\n",
    "\n",
    "results_df[\"perc_improvement\"] = results_df.apply(\n",
    "    lambda row: ((row[\"generated_click_prob\"] - row[\"original_click_prob\"]) / row[\"original_click_prob\"] * 100)\n",
    "    if row[\"original_click_prob\"] > 0 else 0,\n",
    "    axis=1\n",
    ")\n",
    "percentage_improvement = results_df[\"perc_improvement\"].mean()\n",
    "\n",
    "win_rate = (results_df[\"generated_click_prob\"] > results_df[\"original_click_prob\"]).mean() * 100\n",
    "\n",
    "print(\"Mean Click Probability Difference (Generated - Original):\", mean_diff)\n",
    "print(\"Average Percentage Improvement:\", percentage_improvement, \"%\")\n",
    "print(\"Win Rate (% of cases where generated headline wins):\", win_rate, \"%\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average similarity between original and generated headlines: 0.3146\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "csv_path = generated_csv\n",
    "results_df = pd.read_csv(csv_path)\n",
    "\n",
    "def compute_similarity(row):\n",
    "    texts = [row[\"original_headline\"], row[\"cleaned_headline\"]]\n",
    "    vectorizer = TfidfVectorizer().fit(texts)\n",
    "    tfidf_matrix = vectorizer.transform(texts)\n",
    "    sim = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])\n",
    "    return sim[0][0]\n",
    "\n",
    "results_df[\"similarity\"] = results_df.apply(compute_similarity, axis=1)\n",
    "avg_similarity = results_df[\"similarity\"].mean()\n",
    "\n",
    "print(\"Average similarity between original and generated headlines: {:.4f}\".format(avg_similarity))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.read_csv(generated_csv)\n",
    "news_tsv = news_file\n",
    "\n",
    "news_df = pd.read_csv(news_tsv, sep='\\t', header=None, names=[\n",
    "    \"news_id\", \"category\", \"subcategory\", \"title\", \"abstract\", \"url\", \"entity_title\", \"entity_abstract\"\n",
    "])\n",
    "news_df.fillna(\"\", inplace=True)\n",
    "\n",
    "merged_df = results_df.merge(news_df[[\"news_id\", \"category\"]], left_on=\"article_id\", right_on=\"news_id\", how=\"left\")\n",
    "merged_df[\"diff\"] = merged_df[\"generated_click_prob\"] - merged_df[\"original_click_prob\"]\n",
    "\n",
    "merged_df[\"perc_improvement\"] = merged_df.apply(\n",
    "    lambda row: ((row[\"generated_click_prob\"] - row[\"original_click_prob\"]) / row[\"original_click_prob\"] * 100)\n",
    "    if row[\"original_click_prob\"] > 0 else 0,\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "grouped = merged_df.groupby(\"category\")\n",
    "\n",
    "print(\"Metrics by Category:\")\n",
    "for category, group in grouped:\n",
    "    mean_diff = group[\"diff\"].mean()\n",
    "    percentage_improvement = group[\"perc_improvement\"].mean()\n",
    "    win_rate = (group[\"generated_click_prob\"] > group[\"original_click_prob\"]).mean() * 100\n",
    "    \n",
    "    print(\"Category:\", category)\n",
    "    print(\"  Mean Click Probability Difference (Generated - Original): {:.4f}\".format(mean_diff))\n",
    "    print(\"  Average Percentage Improvement: {:.2f}%\".format(percentage_improvement))\n",
    "    print(\"  Win Rate (% of cases where generated headline wins): {:.2f}%\".format(win_rate))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "naml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
